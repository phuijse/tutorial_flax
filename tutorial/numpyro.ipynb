{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import YouTubeVideo\n",
    "from tqdm.notebook import tqdm\n",
    "import corner\n",
    "\n",
    "import torch\n",
    "import pyro\n",
    "print(f\"Torch version: {torch.__version__}\")\n",
    "print(f\"Pyro version: {pyro.__version__}\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from astro_utils import plot_params, plot_lc, plot_lc_folded, featurize_lc, plot_lc_features, make_train_plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pyro basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example: Bayesian linear regression \n",
    "\n",
    "$$\n",
    "y_i = w x_i + b +  \\epsilon_i\n",
    "$$\n",
    "\n",
    "with $N$ observations $(x_i, y_i)$, Gaussian noise and Gaussian priors for $w$ and  $b$\n",
    "\n",
    "The generative process in this case is \n",
    "\n",
    "- Sample $w \\sim \\mathcal{N}(\\mu_w, \\sigma_w^2)$\n",
    "- Sample $b \\sim \\mathcal{N}(\\mu_b, \\sigma_b^2)$\n",
    "- For $i=1,2,\\ldots, N$\n",
    "    - Sample $y_i \\sim \\mathcal{N}(w x_i + b, \\sigma_\\epsilon^2)$\n",
    "    \n",
    "where $\\mu_w, \\sigma_w, \\mu_b, \\sigma_b, \\sigma_\\epsilon$ are hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Writing a model\n",
    "\n",
    "To write the model we use the submodules and primitives\n",
    "\n",
    "- `pyro.distributions` to define prior/likelihoods \n",
    "- `pyro.sample` to define random variables (RV): Expects name, distribution and optionally observations\n",
    "- `pyro.plate` for conditionally independent RV: Expects name, and size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, Uniform\n",
    "\n",
    "def model(x, y=None): \n",
    "    w = pyro.sample(\"w\", Normal(0.0, 1.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 1.0))\n",
    "    s = pyro.sample(\"s\", Uniform(0.0, 1.0))    \n",
    "    with pyro.plate('dataset', size=len(x)):\n",
    "        return pyro.sample(\"y\", Normal(x*w + b, s), obs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use \n",
    "\n",
    "- `pyro.infer.Predictive` \n",
    "\n",
    "to draw samples from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, num_samples=500)\n",
    "\n",
    "hatx = np.linspace(-6, 6, num=100).astype('float32') \n",
    "apriori_samples = predictive(torch.from_numpy(hatx))\n",
    "\n",
    "# Plot samples from the priors\n",
    "figure = corner.corner(np.stack([apriori_samples[var].detach().numpy()[:, 0] for var in ['b', 'w', 's']]).T, \n",
    "                       smooth=1., labels=[\"bias\", \"weight\", \"noise_std\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84], \n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})\n",
    "\n",
    "# Plot posterior predictive of y given x\n",
    "y_trace = apriori_samples[\"y\"].detach().numpy()\n",
    "med = np.median(y_trace, axis=0)\n",
    "qua = np.quantile(y_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(hatx, y_trace.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "In the bayesian setting we want the posterior distribution \n",
    "\n",
    "$$\n",
    "p(\\theta | \\mathcal{D}) = \\frac{p(\\mathcal{D}|\\theta) p(\\theta)}{\\int_\\theta p(\\mathcal{D}|\\theta) p(\\theta)}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{D}$ is our dataset and $\\theta = (w, b)$\n",
    "\n",
    "For complex models the posterior is intractable. So we either do\n",
    "\n",
    "- MCMC: Train a Markov chain to generate samples as if they came from the actual posterior: Sampling based\n",
    "- Variational Inference: Choose a more simple posterior that is similar to the actual posterior: Optimization based\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "Propose an approximate (simple) posterior $q_\\phi(\\theta)$, e.g. factorized Gaussian\n",
    "\n",
    "Optimize $\\phi$ so that $q_\\phi$ approximates $p(\\theta|\\mathcal{D})$\n",
    "\n",
    "This is typically done by maximizing a lower bound on the evidence\n",
    "\n",
    "$$\n",
    "\\mathcal{ELBO}(\\phi) = \\mathbb{E}_{\\theta \\sim q_\\phi}[ \\log p(\\mathcal{D}|\\theta)] - \\text{KL}[q_\\phi(\\theta)|p(\\theta)]\n",
    "$$\n",
    "\n",
    "- Maximize the likelihood of the model\n",
    "- Minimize the distance between the approximate posterior and the prior\n",
    "\n",
    "Once $q$ has been trained we use it as a replacement for $p(\\theta|\\mathcal{D})$ to calculate the **posterior predictive distribution**\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|\\mathbf{x}, \\mathcal{D}) = \\int p(\\mathbf{y}|\\mathbf{x}, \\theta) p(\\theta| \\mathcal{D}) \\,d\\theta\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI with Pyro\n",
    "\n",
    "We use `pyro.infer.SVI` to perform **Stochastic Variational Inference**, which expects\n",
    "\n",
    "- A generative model\n",
    "- An approximate posterior (guide)\n",
    "- Cost function: Typically ELBO\n",
    "- Optimizer: How to optimize the ELBO, typically gradient descent based\n",
    "\n",
    "We can use the `pyro.infer.autoguide` to create approximate posteriors from predefined recipes, for example a factorized Gaussian posterior (`AutoDiagonalNormal`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.enable_validation(True) # Useful for debugging\n",
    "pyro.clear_param_store()\n",
    "\n",
    "# Create a guide (approximate posterior)\n",
    "from pyro.infer.autoguide import AutoDiagonalNormal as approx_posterior\n",
    "guide = approx_posterior(model)\n",
    "\n",
    "\n",
    "# Stochastic Variational Inference\n",
    "svi = pyro.infer.SVI(model=model,  \n",
    "                     guide=guide,\n",
    "                     loss=pyro.infer.Trace_ELBO(), \n",
    "                     optim=pyro.optim.ClippedAdam({\"lr\": 1e-2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets consider a dataset with two observations $\\mathcal{D} = \\{ (-2, -2), (2, 2) \\}$\n",
    "\n",
    "`svi.step(x, y)` performs a gradient ascent step to maximize the ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(10, 2.5), dpi=80, tight_layout=True)\n",
    "\n",
    "nepochs = 1000\n",
    "loss = np.zeros(shape=(nepochs, ))\n",
    "params = np.zeros(shape=(nepochs, 2, 3))\n",
    "\n",
    "# Observed data\n",
    "x = torch.tensor([-2., 2.])\n",
    "y = torch.tensor([-2., 2.])\n",
    "\n",
    "for k in tqdm(range(nepochs)):\n",
    "    loss[k] = svi.step(x, y)\n",
    "    \n",
    "    phi = [param.detach().numpy() for param in guide.parameters()]\n",
    "    params[k, 0, :] = phi[0] # Locations\n",
    "    params[k, 1, :] = phi[1] # Scales    \n",
    "    if np.mod(k, 10) == 0:\n",
    "        plot_params(ax, k+1, loss, params)\n",
    "        fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can use `pyro.infer.Predictive` to draw samples from the model\n",
    "\n",
    "This time we use the guide to sample $w$ and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, \n",
    "                                   guide=guide, \n",
    "                                   num_samples=1000)\n",
    "\n",
    "posterior_samples = predictive(torch.from_numpy(hatx))\n",
    "\n",
    "# Plot posterior of w,  b and s\n",
    "figure = corner.corner(np.stack([posterior_samples[var].detach().numpy()[:, 0] for var in ['b', 'w', 's']]).T, \n",
    "                       smooth=1., labels=[\"bias\", \"weight\", \"noise_std\"], bins=20, \n",
    "                       quantiles=[0.16, 0.5, 0.84],\n",
    "                       show_titles=True, title_kwargs={\"fontsize\": 12})\n",
    "\n",
    "# Plot posterior predictive of y given x\n",
    "y_trace = posterior_samples[\"y\"].detach().numpy()\n",
    "med = np.median(y_trace, axis=0)\n",
    "qua = np.quantile(y_trace, (0.05, 0.95), axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 3), tight_layout=True)\n",
    "ax.plot(hatx, med)\n",
    "ax.fill_between(hatx, qua[0], qua[1], alpha=0.5);\n",
    "\n",
    "ax.errorbar(x.numpy(), y.numpy(), yerr=2*posterior_samples['s'].median().item(), \n",
    "           fmt='none', c='k', zorder=100);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
